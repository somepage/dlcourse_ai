{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "#check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302190\n",
      "Epoch 1, loss: 2.301713\n",
      "Epoch 2, loss: 2.302020\n",
      "Epoch 3, loss: 2.300709\n",
      "Epoch 4, loss: 2.301728\n",
      "Epoch 5, loss: 2.301847\n",
      "Epoch 6, loss: 2.301665\n",
      "Epoch 7, loss: 2.300959\n",
      "Epoch 8, loss: 2.300427\n",
      "Epoch 9, loss: 2.300413\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12618da90>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.300957\n",
      "Epoch 1, loss: 2.301714\n",
      "Epoch 2, loss: 2.301347\n",
      "Epoch 3, loss: 2.302446\n",
      "Epoch 4, loss: 2.301269\n",
      "Epoch 5, loss: 2.300666\n",
      "Epoch 6, loss: 2.300302\n",
      "Epoch 7, loss: 2.300502\n",
      "Epoch 8, loss: 2.301445\n",
      "Epoch 9, loss: 2.301358\n",
      "Epoch 10, loss: 2.300769\n",
      "Epoch 11, loss: 2.301203\n",
      "Epoch 12, loss: 2.300429\n",
      "Epoch 13, loss: 2.301001\n",
      "Epoch 14, loss: 2.301542\n",
      "Epoch 15, loss: 2.301438\n",
      "Epoch 16, loss: 2.300316\n",
      "Epoch 17, loss: 2.301184\n",
      "Epoch 18, loss: 2.301304\n",
      "Epoch 19, loss: 2.301475\n",
      "Epoch 20, loss: 2.301834\n",
      "Epoch 21, loss: 2.301007\n",
      "Epoch 22, loss: 2.301396\n",
      "Epoch 23, loss: 2.301342\n",
      "Epoch 24, loss: 2.301366\n",
      "Epoch 25, loss: 2.300738\n",
      "Epoch 26, loss: 2.300705\n",
      "Epoch 27, loss: 2.301693\n",
      "Epoch 28, loss: 2.300847\n",
      "Epoch 29, loss: 2.300968\n",
      "Epoch 30, loss: 2.300633\n",
      "Epoch 31, loss: 2.300701\n",
      "Epoch 32, loss: 2.300787\n",
      "Epoch 33, loss: 2.301141\n",
      "Epoch 34, loss: 2.301257\n",
      "Epoch 35, loss: 2.300903\n",
      "Epoch 36, loss: 2.301810\n",
      "Epoch 37, loss: 2.302174\n",
      "Epoch 38, loss: 2.300702\n",
      "Epoch 39, loss: 2.300679\n",
      "Epoch 40, loss: 2.300723\n",
      "Epoch 41, loss: 2.301496\n",
      "Epoch 42, loss: 2.301373\n",
      "Epoch 43, loss: 2.301532\n",
      "Epoch 44, loss: 2.301687\n",
      "Epoch 45, loss: 2.301100\n",
      "Epoch 46, loss: 2.300847\n",
      "Epoch 47, loss: 2.301039\n",
      "Epoch 48, loss: 2.301430\n",
      "Epoch 49, loss: 2.301226\n",
      "Epoch 50, loss: 2.301119\n",
      "Epoch 51, loss: 2.301672\n",
      "Epoch 52, loss: 2.301190\n",
      "Epoch 53, loss: 2.301153\n",
      "Epoch 54, loss: 2.300897\n",
      "Epoch 55, loss: 2.300417\n",
      "Epoch 56, loss: 2.301345\n",
      "Epoch 57, loss: 2.300911\n",
      "Epoch 58, loss: 2.301114\n",
      "Epoch 59, loss: 2.301365\n",
      "Epoch 60, loss: 2.300535\n",
      "Epoch 61, loss: 2.301687\n",
      "Epoch 62, loss: 2.301103\n",
      "Epoch 63, loss: 2.300675\n",
      "Epoch 64, loss: 2.301354\n",
      "Epoch 65, loss: 2.301820\n",
      "Epoch 66, loss: 2.301343\n",
      "Epoch 67, loss: 2.302120\n",
      "Epoch 68, loss: 2.300552\n",
      "Epoch 69, loss: 2.300773\n",
      "Epoch 70, loss: 2.300692\n",
      "Epoch 71, loss: 2.301943\n",
      "Epoch 72, loss: 2.300674\n",
      "Epoch 73, loss: 2.300144\n",
      "Epoch 74, loss: 2.301516\n",
      "Epoch 75, loss: 2.301060\n",
      "Epoch 76, loss: 2.300910\n",
      "Epoch 77, loss: 2.301789\n",
      "Epoch 78, loss: 2.301530\n",
      "Epoch 79, loss: 2.300572\n",
      "Epoch 80, loss: 2.301291\n",
      "Epoch 81, loss: 2.301424\n",
      "Epoch 82, loss: 2.300336\n",
      "Epoch 83, loss: 2.301938\n",
      "Epoch 84, loss: 2.301039\n",
      "Epoch 85, loss: 2.301916\n",
      "Epoch 86, loss: 2.301167\n",
      "Epoch 87, loss: 2.301599\n",
      "Epoch 88, loss: 2.301163\n",
      "Epoch 89, loss: 2.301512\n",
      "Epoch 90, loss: 2.300441\n",
      "Epoch 91, loss: 2.301707\n",
      "Epoch 92, loss: 2.301109\n",
      "Epoch 93, loss: 2.300074\n",
      "Epoch 94, loss: 2.301452\n",
      "Epoch 95, loss: 2.300720\n",
      "Epoch 96, loss: 2.301650\n",
      "Epoch 97, loss: 2.300740\n",
      "Epoch 98, loss: 2.301618\n",
      "Epoch 99, loss: 2.301506\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметров.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.299737\n",
      "Epoch 1, loss: 2.299542\n",
      "Epoch 2, loss: 2.298685\n",
      "Epoch 3, loss: 2.298240\n",
      "Epoch 4, loss: 2.299876\n",
      "Epoch 5, loss: 2.297435\n",
      "Epoch 6, loss: 2.298188\n",
      "Epoch 7, loss: 2.296295\n",
      "Epoch 8, loss: 2.292272\n",
      "Epoch 9, loss: 2.292354\n",
      "Epoch 10, loss: 2.294070\n",
      "Epoch 11, loss: 2.292334\n",
      "Epoch 12, loss: 2.293737\n",
      "Epoch 13, loss: 2.283733\n",
      "Epoch 14, loss: 2.282892\n",
      "Epoch 15, loss: 2.285447\n",
      "Epoch 16, loss: 2.287866\n",
      "Epoch 17, loss: 2.285753\n",
      "Epoch 18, loss: 2.285167\n",
      "Epoch 19, loss: 2.285608\n",
      "Epoch 20, loss: 2.289745\n",
      "Epoch 21, loss: 2.283045\n",
      "Epoch 22, loss: 2.282360\n",
      "Epoch 23, loss: 2.282489\n",
      "Epoch 24, loss: 2.276256\n",
      "Epoch 25, loss: 2.282905\n",
      "Epoch 26, loss: 2.276745\n",
      "Epoch 27, loss: 2.282335\n",
      "Epoch 28, loss: 2.285613\n",
      "Epoch 29, loss: 2.280871\n",
      "Epoch 30, loss: 2.272696\n",
      "Epoch 31, loss: 2.272268\n",
      "Epoch 32, loss: 2.278150\n",
      "Epoch 33, loss: 2.275434\n",
      "Epoch 34, loss: 2.273786\n",
      "Epoch 35, loss: 2.273300\n",
      "Epoch 36, loss: 2.265236\n",
      "Epoch 37, loss: 2.267286\n",
      "Epoch 38, loss: 2.265295\n",
      "Epoch 39, loss: 2.266556\n",
      "Epoch 40, loss: 2.271383\n",
      "Epoch 41, loss: 2.273027\n",
      "Epoch 42, loss: 2.267770\n",
      "Epoch 43, loss: 2.263296\n",
      "Epoch 44, loss: 2.263597\n",
      "Epoch 45, loss: 2.261004\n",
      "Epoch 46, loss: 2.259336\n",
      "Epoch 47, loss: 2.265462\n",
      "Epoch 48, loss: 2.261000\n",
      "Epoch 49, loss: 2.265724\n",
      "Epoch 50, loss: 2.268358\n",
      "Epoch 51, loss: 2.263055\n",
      "Epoch 52, loss: 2.275972\n",
      "Epoch 53, loss: 2.256239\n",
      "Epoch 54, loss: 2.266370\n",
      "Epoch 55, loss: 2.259469\n",
      "Epoch 56, loss: 2.262913\n",
      "Epoch 57, loss: 2.250938\n",
      "Epoch 58, loss: 2.251807\n",
      "Epoch 59, loss: 2.255578\n",
      "Epoch 60, loss: 2.266388\n",
      "Epoch 61, loss: 2.256629\n",
      "Epoch 62, loss: 2.255042\n",
      "Epoch 63, loss: 2.256716\n",
      "Epoch 64, loss: 2.255101\n",
      "Epoch 65, loss: 2.260033\n",
      "Epoch 66, loss: 2.244687\n",
      "Epoch 67, loss: 2.249032\n",
      "Epoch 68, loss: 2.252041\n",
      "Epoch 69, loss: 2.247103\n",
      "Epoch 70, loss: 2.260713\n",
      "Epoch 71, loss: 2.268009\n",
      "Epoch 72, loss: 2.259367\n",
      "Epoch 73, loss: 2.237494\n",
      "Epoch 74, loss: 2.254041\n",
      "Epoch 75, loss: 2.236158\n",
      "Epoch 76, loss: 2.250794\n",
      "Epoch 77, loss: 2.263524\n",
      "Epoch 78, loss: 2.253343\n",
      "Epoch 79, loss: 2.258135\n",
      "Epoch 80, loss: 2.255566\n",
      "Epoch 81, loss: 2.240346\n",
      "Epoch 82, loss: 2.240738\n",
      "Epoch 83, loss: 2.260784\n",
      "Epoch 84, loss: 2.245902\n",
      "Epoch 85, loss: 2.254729\n",
      "Epoch 86, loss: 2.229284\n",
      "Epoch 87, loss: 2.254522\n",
      "Epoch 88, loss: 2.231822\n",
      "Epoch 89, loss: 2.242139\n",
      "Epoch 90, loss: 2.245997\n",
      "Epoch 91, loss: 2.240302\n",
      "Epoch 92, loss: 2.243605\n",
      "Epoch 93, loss: 2.228100\n",
      "Epoch 94, loss: 2.255602\n",
      "Epoch 95, loss: 2.221815\n",
      "Epoch 96, loss: 2.236601\n",
      "Epoch 97, loss: 2.241256\n",
      "Epoch 98, loss: 2.233867\n",
      "Epoch 99, loss: 2.239344\n",
      "Accuracy after training for 100 epochs:  0.216\n",
      "Epoch 0, loss: 2.218853\n",
      "Epoch 1, loss: 2.233984\n",
      "Epoch 2, loss: 2.233858\n",
      "Epoch 3, loss: 2.226038\n",
      "Epoch 4, loss: 2.227810\n",
      "Epoch 5, loss: 2.217775\n",
      "Epoch 6, loss: 2.246047\n",
      "Epoch 7, loss: 2.209360\n",
      "Epoch 8, loss: 2.222696\n",
      "Epoch 9, loss: 2.239565\n",
      "Epoch 10, loss: 2.245228\n",
      "Epoch 11, loss: 2.237351\n",
      "Epoch 12, loss: 2.235362\n",
      "Epoch 13, loss: 2.214404\n",
      "Epoch 14, loss: 2.240662\n",
      "Epoch 15, loss: 2.231716\n",
      "Epoch 16, loss: 2.229215\n",
      "Epoch 17, loss: 2.248480\n",
      "Epoch 18, loss: 2.248650\n",
      "Epoch 19, loss: 2.233390\n",
      "Epoch 20, loss: 2.210600\n",
      "Epoch 21, loss: 2.218545\n",
      "Epoch 22, loss: 2.221301\n",
      "Epoch 23, loss: 2.218397\n",
      "Epoch 24, loss: 2.220420\n",
      "Epoch 25, loss: 2.213095\n",
      "Epoch 26, loss: 2.231692\n",
      "Epoch 27, loss: 2.226846\n",
      "Epoch 28, loss: 2.238407\n",
      "Epoch 29, loss: 2.210058\n",
      "Epoch 30, loss: 2.234288\n",
      "Epoch 31, loss: 2.233349\n",
      "Epoch 32, loss: 2.236222\n",
      "Epoch 33, loss: 2.226093\n",
      "Epoch 34, loss: 2.214432\n",
      "Epoch 35, loss: 2.244599\n",
      "Epoch 36, loss: 2.235955\n",
      "Epoch 37, loss: 2.215896\n",
      "Epoch 38, loss: 2.222647\n",
      "Epoch 39, loss: 2.225512\n",
      "Epoch 40, loss: 2.237322\n",
      "Epoch 41, loss: 2.216421\n",
      "Epoch 42, loss: 2.228548\n",
      "Epoch 43, loss: 2.217814\n",
      "Epoch 44, loss: 2.226567\n",
      "Epoch 45, loss: 2.202150\n",
      "Epoch 46, loss: 2.204517\n",
      "Epoch 47, loss: 2.199184\n",
      "Epoch 48, loss: 2.205577\n",
      "Epoch 49, loss: 2.203927\n",
      "Epoch 50, loss: 2.223026\n",
      "Epoch 51, loss: 2.211162\n",
      "Epoch 52, loss: 2.208900\n",
      "Epoch 53, loss: 2.189352\n",
      "Epoch 54, loss: 2.219177\n",
      "Epoch 55, loss: 2.207424\n",
      "Epoch 56, loss: 2.225020\n",
      "Epoch 57, loss: 2.213986\n",
      "Epoch 58, loss: 2.208651\n",
      "Epoch 59, loss: 2.228082\n",
      "Epoch 60, loss: 2.218411\n",
      "Epoch 61, loss: 2.218071\n",
      "Epoch 62, loss: 2.224215\n",
      "Epoch 63, loss: 2.234676\n",
      "Epoch 64, loss: 2.203616\n",
      "Epoch 65, loss: 2.225994\n",
      "Epoch 66, loss: 2.213499\n",
      "Epoch 67, loss: 2.206517\n",
      "Epoch 68, loss: 2.209888\n",
      "Epoch 69, loss: 2.215594\n",
      "Epoch 70, loss: 2.224825\n",
      "Epoch 71, loss: 2.204458\n",
      "Epoch 72, loss: 2.207067\n",
      "Epoch 73, loss: 2.207007\n",
      "Epoch 74, loss: 2.213827\n",
      "Epoch 75, loss: 2.202861\n",
      "Epoch 76, loss: 2.231839\n",
      "Epoch 77, loss: 2.187165\n",
      "Epoch 78, loss: 2.219957\n",
      "Epoch 79, loss: 2.190677\n",
      "Epoch 80, loss: 2.201266\n",
      "Epoch 81, loss: 2.203284\n",
      "Epoch 82, loss: 2.217540\n",
      "Epoch 83, loss: 2.224168\n",
      "Epoch 84, loss: 2.183884\n",
      "Epoch 85, loss: 2.211042\n",
      "Epoch 86, loss: 2.232167\n",
      "Epoch 87, loss: 2.181296\n",
      "Epoch 88, loss: 2.221605\n",
      "Epoch 89, loss: 2.217413\n",
      "Epoch 90, loss: 2.202574\n",
      "Epoch 91, loss: 2.220203\n",
      "Epoch 92, loss: 2.195420\n",
      "Epoch 93, loss: 2.184972\n",
      "Epoch 94, loss: 2.226240\n",
      "Epoch 95, loss: 2.218608\n",
      "Epoch 96, loss: 2.209314\n",
      "Epoch 97, loss: 2.227381\n",
      "Epoch 98, loss: 2.231000\n",
      "Epoch 99, loss: 2.196268\n",
      "Accuracy after training for 100 epochs:  0.227\n",
      "Epoch 0, loss: 2.198909\n",
      "Epoch 1, loss: 2.195730\n",
      "Epoch 2, loss: 2.204513\n",
      "Epoch 3, loss: 2.194235\n",
      "Epoch 4, loss: 2.229849\n",
      "Epoch 5, loss: 2.178669\n",
      "Epoch 6, loss: 2.199194\n",
      "Epoch 7, loss: 2.207645\n",
      "Epoch 8, loss: 2.210857\n",
      "Epoch 9, loss: 2.206487\n",
      "Epoch 10, loss: 2.171650\n",
      "Epoch 11, loss: 2.225198\n",
      "Epoch 12, loss: 2.181055\n",
      "Epoch 13, loss: 2.231280\n",
      "Epoch 14, loss: 2.193648\n",
      "Epoch 15, loss: 2.186753\n",
      "Epoch 16, loss: 2.163681\n",
      "Epoch 17, loss: 2.203532\n",
      "Epoch 18, loss: 2.195766\n",
      "Epoch 19, loss: 2.190144\n",
      "Epoch 20, loss: 2.194218\n",
      "Epoch 21, loss: 2.200666\n",
      "Epoch 22, loss: 2.209814\n",
      "Epoch 23, loss: 2.194322\n",
      "Epoch 24, loss: 2.168183\n",
      "Epoch 25, loss: 2.202828\n",
      "Epoch 26, loss: 2.218656\n",
      "Epoch 27, loss: 2.182171\n",
      "Epoch 28, loss: 2.201248\n",
      "Epoch 29, loss: 2.199195\n",
      "Epoch 30, loss: 2.168869\n",
      "Epoch 31, loss: 2.192455\n",
      "Epoch 32, loss: 2.202418\n",
      "Epoch 33, loss: 2.199893\n",
      "Epoch 34, loss: 2.213446\n",
      "Epoch 35, loss: 2.175092\n",
      "Epoch 36, loss: 2.213682\n",
      "Epoch 37, loss: 2.175097\n",
      "Epoch 38, loss: 2.203029\n",
      "Epoch 39, loss: 2.203494\n",
      "Epoch 40, loss: 2.190945\n",
      "Epoch 41, loss: 2.178243\n",
      "Epoch 42, loss: 2.217719\n",
      "Epoch 43, loss: 2.190169\n",
      "Epoch 44, loss: 2.179031\n",
      "Epoch 45, loss: 2.166639\n",
      "Epoch 46, loss: 2.177283\n",
      "Epoch 47, loss: 2.207491\n",
      "Epoch 48, loss: 2.179620\n",
      "Epoch 49, loss: 2.191110\n",
      "Epoch 50, loss: 2.186153\n",
      "Epoch 51, loss: 2.196051\n",
      "Epoch 52, loss: 2.167172\n",
      "Epoch 53, loss: 2.188099\n",
      "Epoch 54, loss: 2.200036\n",
      "Epoch 55, loss: 2.166929\n",
      "Epoch 56, loss: 2.200131\n",
      "Epoch 57, loss: 2.184352\n",
      "Epoch 58, loss: 2.193147\n",
      "Epoch 59, loss: 2.203023\n",
      "Epoch 60, loss: 2.150989\n",
      "Epoch 61, loss: 2.213830\n",
      "Epoch 62, loss: 2.192460\n",
      "Epoch 63, loss: 2.162480\n",
      "Epoch 64, loss: 2.165688\n",
      "Epoch 65, loss: 2.223687\n",
      "Epoch 66, loss: 2.143362\n",
      "Epoch 67, loss: 2.164555\n",
      "Epoch 68, loss: 2.176912\n",
      "Epoch 69, loss: 2.180883\n",
      "Epoch 70, loss: 2.168999\n",
      "Epoch 71, loss: 2.171926\n",
      "Epoch 72, loss: 2.164228\n",
      "Epoch 73, loss: 2.200066\n",
      "Epoch 74, loss: 2.156019\n",
      "Epoch 75, loss: 2.191456\n",
      "Epoch 76, loss: 2.161492\n",
      "Epoch 77, loss: 2.212427\n",
      "Epoch 78, loss: 2.178948\n",
      "Epoch 79, loss: 2.184260\n",
      "Epoch 80, loss: 2.222009\n",
      "Epoch 81, loss: 2.180268\n",
      "Epoch 82, loss: 2.198016\n",
      "Epoch 83, loss: 2.172277\n",
      "Epoch 84, loss: 2.150939\n",
      "Epoch 85, loss: 2.200755\n",
      "Epoch 86, loss: 2.200512\n",
      "Epoch 87, loss: 2.198528\n",
      "Epoch 88, loss: 2.177706\n",
      "Epoch 89, loss: 2.166654\n",
      "Epoch 90, loss: 2.184512\n",
      "Epoch 91, loss: 2.141580\n",
      "Epoch 92, loss: 2.188031\n",
      "Epoch 93, loss: 2.167686\n",
      "Epoch 94, loss: 2.180933\n",
      "Epoch 95, loss: 2.204762\n",
      "Epoch 96, loss: 2.208472\n",
      "Epoch 97, loss: 2.149903\n",
      "Epoch 98, loss: 2.214439\n",
      "Epoch 99, loss: 2.207803\n",
      "Accuracy after training for 100 epochs:  0.225\n",
      "Epoch 0, loss: 2.176894\n",
      "Epoch 1, loss: 2.191982\n",
      "Epoch 2, loss: 2.144003\n",
      "Epoch 3, loss: 2.174272\n",
      "Epoch 4, loss: 2.158155\n",
      "Epoch 5, loss: 2.189125\n",
      "Epoch 6, loss: 2.162085\n",
      "Epoch 7, loss: 2.167361\n",
      "Epoch 8, loss: 2.180346\n",
      "Epoch 9, loss: 2.184925\n",
      "Epoch 10, loss: 2.186279\n",
      "Epoch 11, loss: 2.175976\n",
      "Epoch 12, loss: 2.187179\n",
      "Epoch 13, loss: 2.185076\n",
      "Epoch 14, loss: 2.180727\n",
      "Epoch 15, loss: 2.192484\n",
      "Epoch 16, loss: 2.159231\n",
      "Epoch 17, loss: 2.176539\n",
      "Epoch 18, loss: 2.198710\n",
      "Epoch 19, loss: 2.200494\n",
      "Epoch 20, loss: 2.233634\n",
      "Epoch 21, loss: 2.251150\n",
      "Epoch 22, loss: 2.188971\n",
      "Epoch 23, loss: 2.166031\n",
      "Epoch 24, loss: 2.190675\n",
      "Epoch 25, loss: 2.219475\n",
      "Epoch 26, loss: 2.230549\n",
      "Epoch 27, loss: 2.148230\n",
      "Epoch 28, loss: 2.215585\n",
      "Epoch 29, loss: 2.192176\n",
      "Epoch 30, loss: 2.160127\n",
      "Epoch 31, loss: 2.181180\n",
      "Epoch 32, loss: 2.227018\n",
      "Epoch 33, loss: 2.163002\n",
      "Epoch 34, loss: 2.200252\n",
      "Epoch 35, loss: 2.176206\n",
      "Epoch 36, loss: 2.182594\n",
      "Epoch 37, loss: 2.200515\n",
      "Epoch 38, loss: 2.142840\n",
      "Epoch 39, loss: 2.178078\n",
      "Epoch 40, loss: 2.157935\n",
      "Epoch 41, loss: 2.202078\n",
      "Epoch 42, loss: 2.157138\n",
      "Epoch 43, loss: 2.169129\n",
      "Epoch 44, loss: 2.192875\n",
      "Epoch 45, loss: 2.191213\n",
      "Epoch 46, loss: 2.166514\n",
      "Epoch 47, loss: 2.197665\n",
      "Epoch 48, loss: 2.178432\n",
      "Epoch 49, loss: 2.155628\n",
      "Epoch 50, loss: 2.192547\n",
      "Epoch 51, loss: 2.159167\n",
      "Epoch 52, loss: 2.184352\n",
      "Epoch 53, loss: 2.183908\n",
      "Epoch 54, loss: 2.181554\n",
      "Epoch 55, loss: 2.196629\n",
      "Epoch 56, loss: 2.176769\n",
      "Epoch 57, loss: 2.172725\n",
      "Epoch 58, loss: 2.152845\n",
      "Epoch 59, loss: 2.146143\n",
      "Epoch 60, loss: 2.180418\n",
      "Epoch 61, loss: 2.192343\n",
      "Epoch 62, loss: 2.159727\n",
      "Epoch 63, loss: 2.191582\n",
      "Epoch 64, loss: 2.161881\n",
      "Epoch 65, loss: 2.187429\n",
      "Epoch 66, loss: 2.158795\n",
      "Epoch 67, loss: 2.200341\n",
      "Epoch 68, loss: 2.175384\n",
      "Epoch 69, loss: 2.180385\n",
      "Epoch 70, loss: 2.208754\n",
      "Epoch 71, loss: 2.160935\n",
      "Epoch 72, loss: 2.198410\n",
      "Epoch 73, loss: 2.183591\n",
      "Epoch 74, loss: 2.246579\n",
      "Epoch 75, loss: 2.138756\n",
      "Epoch 76, loss: 2.132156\n",
      "Epoch 77, loss: 2.192257\n",
      "Epoch 78, loss: 2.141485\n",
      "Epoch 79, loss: 2.174892\n",
      "Epoch 80, loss: 2.140880\n",
      "Epoch 81, loss: 2.185632\n",
      "Epoch 82, loss: 2.191893\n",
      "Epoch 83, loss: 2.181579\n",
      "Epoch 84, loss: 2.183193\n",
      "Epoch 85, loss: 2.187202\n",
      "Epoch 86, loss: 2.177099\n",
      "Epoch 87, loss: 2.197882\n",
      "Epoch 88, loss: 2.157903\n",
      "Epoch 89, loss: 2.170658\n",
      "Epoch 90, loss: 2.166875\n",
      "Epoch 91, loss: 2.188206\n",
      "Epoch 92, loss: 2.158325\n",
      "Epoch 93, loss: 2.169592\n",
      "Epoch 94, loss: 2.167731\n",
      "Epoch 95, loss: 2.191048\n",
      "Epoch 96, loss: 2.154618\n",
      "Epoch 97, loss: 2.198211\n",
      "Epoch 98, loss: 2.204298\n",
      "Epoch 99, loss: 2.175563\n",
      "Accuracy after training for 100 epochs:  0.224\n",
      "Epoch 0, loss: 2.155671\n",
      "Epoch 1, loss: 2.180329\n",
      "Epoch 2, loss: 2.168128\n",
      "Epoch 3, loss: 2.138658\n",
      "Epoch 4, loss: 2.142528\n",
      "Epoch 5, loss: 2.166553\n",
      "Epoch 6, loss: 2.180775\n",
      "Epoch 7, loss: 2.174656\n",
      "Epoch 8, loss: 2.176777\n",
      "Epoch 9, loss: 2.179803\n",
      "Epoch 10, loss: 2.182700\n",
      "Epoch 11, loss: 2.189277\n",
      "Epoch 12, loss: 2.192664\n",
      "Epoch 13, loss: 2.187134\n",
      "Epoch 14, loss: 2.181730\n",
      "Epoch 15, loss: 2.158646\n",
      "Epoch 16, loss: 2.209185\n",
      "Epoch 17, loss: 2.185226\n",
      "Epoch 18, loss: 2.227789\n",
      "Epoch 19, loss: 2.214400\n",
      "Epoch 20, loss: 2.178587\n",
      "Epoch 21, loss: 2.149007\n",
      "Epoch 22, loss: 2.214165\n",
      "Epoch 23, loss: 2.161512\n",
      "Epoch 24, loss: 2.152985\n",
      "Epoch 25, loss: 2.189145\n",
      "Epoch 26, loss: 2.156502\n",
      "Epoch 27, loss: 2.147004\n",
      "Epoch 28, loss: 2.184591\n",
      "Epoch 29, loss: 2.181806\n",
      "Epoch 30, loss: 2.191507\n",
      "Epoch 31, loss: 2.205525\n",
      "Epoch 32, loss: 2.192207\n",
      "Epoch 33, loss: 2.161444\n",
      "Epoch 34, loss: 2.166896\n",
      "Epoch 35, loss: 2.150040\n",
      "Epoch 36, loss: 2.177080\n",
      "Epoch 37, loss: 2.144989\n",
      "Epoch 38, loss: 2.180550\n",
      "Epoch 39, loss: 2.176428\n",
      "Epoch 40, loss: 2.204512\n",
      "Epoch 41, loss: 2.203552\n",
      "Epoch 42, loss: 2.167604\n",
      "Epoch 43, loss: 2.170572\n",
      "Epoch 44, loss: 2.190473\n",
      "Epoch 45, loss: 2.186143\n",
      "Epoch 46, loss: 2.185704\n",
      "Epoch 47, loss: 2.193194\n",
      "Epoch 48, loss: 2.181666\n",
      "Epoch 49, loss: 2.125551\n",
      "Epoch 50, loss: 2.164644\n",
      "Epoch 51, loss: 2.166203\n",
      "Epoch 52, loss: 2.182139\n",
      "Epoch 53, loss: 2.130207\n",
      "Epoch 54, loss: 2.184253\n",
      "Epoch 55, loss: 2.198273\n",
      "Epoch 56, loss: 2.149085\n",
      "Epoch 57, loss: 2.151649\n",
      "Epoch 58, loss: 2.190531\n",
      "Epoch 59, loss: 2.176007\n",
      "Epoch 60, loss: 2.192659\n",
      "Epoch 61, loss: 2.174914\n",
      "Epoch 62, loss: 2.172072\n",
      "Epoch 63, loss: 2.168947\n",
      "Epoch 64, loss: 2.165051\n",
      "Epoch 65, loss: 2.164794\n",
      "Epoch 66, loss: 2.171247\n",
      "Epoch 67, loss: 2.153753\n",
      "Epoch 68, loss: 2.221096\n",
      "Epoch 69, loss: 2.195301\n",
      "Epoch 70, loss: 2.142624\n",
      "Epoch 71, loss: 2.178149\n",
      "Epoch 72, loss: 2.200252\n",
      "Epoch 73, loss: 2.188004\n",
      "Epoch 74, loss: 2.197894\n",
      "Epoch 75, loss: 2.152406\n",
      "Epoch 76, loss: 2.191037\n",
      "Epoch 77, loss: 2.192995\n",
      "Epoch 78, loss: 2.194494\n",
      "Epoch 79, loss: 2.157701\n",
      "Epoch 80, loss: 2.167189\n",
      "Epoch 81, loss: 2.162945\n",
      "Epoch 82, loss: 2.147584\n",
      "Epoch 83, loss: 2.158029\n",
      "Epoch 84, loss: 2.195541\n",
      "Epoch 85, loss: 2.217376\n",
      "Epoch 86, loss: 2.141704\n",
      "Epoch 87, loss: 2.184309\n",
      "Epoch 88, loss: 2.157164\n",
      "Epoch 89, loss: 2.160428\n",
      "Epoch 90, loss: 2.175760\n",
      "Epoch 91, loss: 2.165893\n",
      "Epoch 92, loss: 2.182428\n",
      "Epoch 93, loss: 2.167847\n",
      "Epoch 94, loss: 2.190674\n",
      "Epoch 95, loss: 2.135831\n",
      "Epoch 96, loss: 2.157204\n",
      "Epoch 97, loss: 2.220029\n",
      "Epoch 98, loss: 2.153433\n",
      "Epoch 99, loss: 2.190125\n",
      "Accuracy after training for 100 epochs:  0.225\n",
      "Epoch 0, loss: 2.115898\n",
      "Epoch 1, loss: 2.165194\n",
      "Epoch 2, loss: 2.148917\n",
      "Epoch 3, loss: 2.176120\n",
      "Epoch 4, loss: 2.162048\n",
      "Epoch 5, loss: 2.136986\n",
      "Epoch 6, loss: 2.143720\n",
      "Epoch 7, loss: 2.194642\n",
      "Epoch 8, loss: 2.184057\n",
      "Epoch 9, loss: 2.190028\n",
      "Epoch 10, loss: 2.186086\n",
      "Epoch 11, loss: 2.186541\n",
      "Epoch 12, loss: 2.185423\n",
      "Epoch 13, loss: 2.175351\n",
      "Epoch 14, loss: 2.194550\n",
      "Epoch 15, loss: 2.213177\n",
      "Epoch 16, loss: 2.173291\n",
      "Epoch 17, loss: 2.189871\n",
      "Epoch 18, loss: 2.146615\n",
      "Epoch 19, loss: 2.184616\n",
      "Epoch 20, loss: 2.204242\n",
      "Epoch 21, loss: 2.157147\n",
      "Epoch 22, loss: 2.164144\n",
      "Epoch 23, loss: 2.194070\n",
      "Epoch 24, loss: 2.164894\n",
      "Epoch 25, loss: 2.154554\n",
      "Epoch 26, loss: 2.150616\n",
      "Epoch 27, loss: 2.173328\n",
      "Epoch 28, loss: 2.181605\n",
      "Epoch 29, loss: 2.170269\n",
      "Epoch 30, loss: 2.206839\n",
      "Epoch 31, loss: 2.158827\n",
      "Epoch 32, loss: 2.162283\n",
      "Epoch 33, loss: 2.203214\n",
      "Epoch 34, loss: 2.199550\n",
      "Epoch 35, loss: 2.181779\n",
      "Epoch 36, loss: 2.133347\n",
      "Epoch 37, loss: 2.179509\n",
      "Epoch 38, loss: 2.171659\n",
      "Epoch 39, loss: 2.138564\n",
      "Epoch 40, loss: 2.199253\n",
      "Epoch 41, loss: 2.175790\n",
      "Epoch 42, loss: 2.172037\n",
      "Epoch 43, loss: 2.172560\n",
      "Epoch 44, loss: 2.145498\n",
      "Epoch 45, loss: 2.166361\n",
      "Epoch 46, loss: 2.161158\n",
      "Epoch 47, loss: 2.171753\n",
      "Epoch 48, loss: 2.182596\n",
      "Epoch 49, loss: 2.162972\n",
      "Epoch 50, loss: 2.213908\n",
      "Epoch 51, loss: 2.195468\n",
      "Epoch 52, loss: 2.157039\n",
      "Epoch 53, loss: 2.206635\n",
      "Epoch 54, loss: 2.164331\n",
      "Epoch 55, loss: 2.161859\n",
      "Epoch 56, loss: 2.166851\n",
      "Epoch 57, loss: 2.230834\n",
      "Epoch 58, loss: 2.190165\n",
      "Epoch 59, loss: 2.185147\n",
      "Epoch 60, loss: 2.150219\n",
      "Epoch 61, loss: 2.155653\n",
      "Epoch 62, loss: 2.187010\n",
      "Epoch 63, loss: 2.167089\n",
      "Epoch 64, loss: 2.135318\n",
      "Epoch 65, loss: 2.178907\n",
      "Epoch 66, loss: 2.165857\n",
      "Epoch 67, loss: 2.170045\n",
      "Epoch 68, loss: 2.155163\n",
      "Epoch 69, loss: 2.198389\n",
      "Epoch 70, loss: 2.194190\n",
      "Epoch 71, loss: 2.187536\n",
      "Epoch 72, loss: 2.175019\n",
      "Epoch 73, loss: 2.161014\n",
      "Epoch 74, loss: 2.206931\n",
      "Epoch 75, loss: 2.193168\n",
      "Epoch 76, loss: 2.202087\n",
      "Epoch 77, loss: 2.189926\n",
      "Epoch 78, loss: 2.163655\n",
      "Epoch 79, loss: 2.226779\n",
      "Epoch 80, loss: 2.186606\n",
      "Epoch 81, loss: 2.138207\n",
      "Epoch 82, loss: 2.202070\n",
      "Epoch 83, loss: 2.158168\n",
      "Epoch 84, loss: 2.181542\n",
      "Epoch 85, loss: 2.163246\n",
      "Epoch 86, loss: 2.166892\n",
      "Epoch 87, loss: 2.165821\n",
      "Epoch 88, loss: 2.175115\n",
      "Epoch 89, loss: 2.191332\n",
      "Epoch 90, loss: 2.171183\n",
      "Epoch 91, loss: 2.193899\n",
      "Epoch 92, loss: 2.196116\n",
      "Epoch 93, loss: 2.161665\n",
      "Epoch 94, loss: 2.156408\n",
      "Epoch 95, loss: 2.185138\n",
      "Epoch 96, loss: 2.163117\n",
      "Epoch 97, loss: 2.176055\n",
      "Epoch 98, loss: 2.146285\n",
      "Epoch 99, loss: 2.205350\n",
      "Accuracy after training for 100 epochs:  0.226\n",
      "Epoch 0, loss: 2.192789\n",
      "Epoch 1, loss: 2.163982\n",
      "Epoch 2, loss: 2.166501\n",
      "Epoch 3, loss: 2.187183\n",
      "Epoch 4, loss: 2.195657\n",
      "Epoch 5, loss: 2.186149\n",
      "Epoch 6, loss: 2.177520\n",
      "Epoch 7, loss: 2.178325\n",
      "Epoch 8, loss: 2.172907\n",
      "Epoch 9, loss: 2.177553\n",
      "Epoch 10, loss: 2.180800\n",
      "Epoch 11, loss: 2.191103\n",
      "Epoch 12, loss: 2.161034\n",
      "Epoch 13, loss: 2.151592\n",
      "Epoch 14, loss: 2.203423\n",
      "Epoch 15, loss: 2.203097\n",
      "Epoch 16, loss: 2.168137\n",
      "Epoch 17, loss: 2.208998\n",
      "Epoch 18, loss: 2.184081\n",
      "Epoch 19, loss: 2.167579\n",
      "Epoch 20, loss: 2.175935\n",
      "Epoch 21, loss: 2.198223\n",
      "Epoch 22, loss: 2.145962\n",
      "Epoch 23, loss: 2.159819\n",
      "Epoch 24, loss: 2.181040\n",
      "Epoch 25, loss: 2.178314\n",
      "Epoch 26, loss: 2.130750\n",
      "Epoch 27, loss: 2.201097\n",
      "Epoch 28, loss: 2.175219\n",
      "Epoch 29, loss: 2.157992\n",
      "Epoch 30, loss: 2.177431\n",
      "Epoch 31, loss: 2.136961\n",
      "Epoch 32, loss: 2.143192\n",
      "Epoch 33, loss: 2.177797\n",
      "Epoch 34, loss: 2.166973\n",
      "Epoch 35, loss: 2.191752\n",
      "Epoch 36, loss: 2.177914\n",
      "Epoch 37, loss: 2.182481\n",
      "Epoch 38, loss: 2.168442\n",
      "Epoch 39, loss: 2.149373\n",
      "Epoch 40, loss: 2.172192\n",
      "Epoch 41, loss: 2.187717\n",
      "Epoch 42, loss: 2.203961\n",
      "Epoch 43, loss: 2.171977\n",
      "Epoch 44, loss: 2.212289\n",
      "Epoch 45, loss: 2.182503\n",
      "Epoch 46, loss: 2.165089\n",
      "Epoch 47, loss: 2.196075\n",
      "Epoch 48, loss: 2.172201\n",
      "Epoch 49, loss: 2.171220\n",
      "Epoch 50, loss: 2.184069\n",
      "Epoch 51, loss: 2.183317\n",
      "Epoch 52, loss: 2.189955\n",
      "Epoch 53, loss: 2.172624\n",
      "Epoch 54, loss: 2.183359\n",
      "Epoch 55, loss: 2.162782\n",
      "Epoch 56, loss: 2.205604\n",
      "Epoch 57, loss: 2.203200\n",
      "Epoch 58, loss: 2.177747\n",
      "Epoch 59, loss: 2.224836\n",
      "Epoch 60, loss: 2.191575\n",
      "Epoch 61, loss: 2.163353\n",
      "Epoch 62, loss: 2.168617\n",
      "Epoch 63, loss: 2.214979\n",
      "Epoch 64, loss: 2.126952\n",
      "Epoch 65, loss: 2.170116\n",
      "Epoch 66, loss: 2.175948\n",
      "Epoch 67, loss: 2.188165\n",
      "Epoch 68, loss: 2.173387\n",
      "Epoch 69, loss: 2.175564\n",
      "Epoch 70, loss: 2.180948\n",
      "Epoch 71, loss: 2.161763\n",
      "Epoch 72, loss: 2.155796\n",
      "Epoch 73, loss: 2.165025\n",
      "Epoch 74, loss: 2.180977\n",
      "Epoch 75, loss: 2.182644\n",
      "Epoch 76, loss: 2.182763\n",
      "Epoch 77, loss: 2.157999\n",
      "Epoch 78, loss: 2.181699\n",
      "Epoch 79, loss: 2.150028\n",
      "Epoch 80, loss: 2.173554\n",
      "Epoch 81, loss: 2.174099\n",
      "Epoch 82, loss: 2.183599\n",
      "Epoch 83, loss: 2.182979\n",
      "Epoch 84, loss: 2.190242\n",
      "Epoch 85, loss: 2.165211\n",
      "Epoch 86, loss: 2.218490\n",
      "Epoch 87, loss: 2.208081\n",
      "Epoch 88, loss: 2.169638\n",
      "Epoch 89, loss: 2.171798\n",
      "Epoch 90, loss: 2.153585\n",
      "Epoch 91, loss: 2.146389\n",
      "Epoch 92, loss: 2.166766\n",
      "Epoch 93, loss: 2.184382\n",
      "Epoch 94, loss: 2.200777\n",
      "Epoch 95, loss: 2.182346\n",
      "Epoch 96, loss: 2.201038\n",
      "Epoch 97, loss: 2.174555\n",
      "Epoch 98, loss: 2.167608\n",
      "Epoch 99, loss: 2.156864\n",
      "Accuracy after training for 100 epochs:  0.226\n",
      "Epoch 0, loss: 2.148363\n",
      "Epoch 1, loss: 2.160554\n",
      "Epoch 2, loss: 2.172067\n",
      "Epoch 3, loss: 2.148505\n",
      "Epoch 4, loss: 2.218320\n",
      "Epoch 5, loss: 2.214917\n",
      "Epoch 6, loss: 2.204552\n",
      "Epoch 7, loss: 2.201295\n",
      "Epoch 8, loss: 2.160209\n",
      "Epoch 9, loss: 2.128793\n",
      "Epoch 10, loss: 2.196751\n",
      "Epoch 11, loss: 2.236459\n",
      "Epoch 12, loss: 2.189487\n",
      "Epoch 13, loss: 2.154858\n",
      "Epoch 14, loss: 2.173329\n",
      "Epoch 15, loss: 2.160900\n",
      "Epoch 16, loss: 2.224397\n",
      "Epoch 17, loss: 2.179385\n",
      "Epoch 18, loss: 2.213502\n",
      "Epoch 19, loss: 2.205079\n",
      "Epoch 20, loss: 2.222925\n",
      "Epoch 21, loss: 2.171995\n",
      "Epoch 22, loss: 2.156884\n",
      "Epoch 23, loss: 2.167907\n",
      "Epoch 24, loss: 2.168897\n",
      "Epoch 25, loss: 2.194636\n",
      "Epoch 26, loss: 2.189146\n",
      "Epoch 27, loss: 2.199615\n",
      "Epoch 28, loss: 2.188077\n",
      "Epoch 29, loss: 2.154375\n",
      "Epoch 30, loss: 2.205830\n",
      "Epoch 31, loss: 2.201419\n",
      "Epoch 32, loss: 2.202849\n",
      "Epoch 33, loss: 2.140834\n",
      "Epoch 34, loss: 2.168588\n",
      "Epoch 35, loss: 2.130789\n",
      "Epoch 36, loss: 2.138352\n",
      "Epoch 37, loss: 2.182890\n",
      "Epoch 38, loss: 2.188916\n",
      "Epoch 39, loss: 2.173539\n",
      "Epoch 40, loss: 2.226168\n",
      "Epoch 41, loss: 2.136607\n",
      "Epoch 42, loss: 2.190813\n",
      "Epoch 43, loss: 2.167371\n",
      "Epoch 44, loss: 2.157644\n",
      "Epoch 45, loss: 2.148003\n",
      "Epoch 46, loss: 2.196262\n",
      "Epoch 47, loss: 2.155444\n",
      "Epoch 48, loss: 2.148839\n",
      "Epoch 49, loss: 2.161428\n",
      "Epoch 50, loss: 2.150365\n",
      "Epoch 51, loss: 2.168530\n",
      "Epoch 52, loss: 2.166455\n",
      "Epoch 53, loss: 2.201178\n",
      "Epoch 54, loss: 2.177953\n",
      "Epoch 55, loss: 2.158973\n",
      "Epoch 56, loss: 2.172871\n",
      "Epoch 57, loss: 2.201423\n",
      "Epoch 58, loss: 2.159023\n",
      "Epoch 59, loss: 2.188356\n",
      "Epoch 60, loss: 2.182684\n",
      "Epoch 61, loss: 2.187162\n",
      "Epoch 62, loss: 2.169215\n",
      "Epoch 63, loss: 2.126380\n",
      "Epoch 64, loss: 2.180838\n",
      "Epoch 65, loss: 2.138296\n",
      "Epoch 66, loss: 2.131555\n",
      "Epoch 67, loss: 2.140424\n",
      "Epoch 68, loss: 2.143485\n",
      "Epoch 69, loss: 2.178507\n",
      "Epoch 70, loss: 2.162267\n",
      "Epoch 71, loss: 2.172008\n",
      "Epoch 72, loss: 2.148093\n",
      "Epoch 73, loss: 2.179137\n",
      "Epoch 74, loss: 2.177529\n",
      "Epoch 75, loss: 2.188660\n",
      "Epoch 76, loss: 2.176424\n",
      "Epoch 77, loss: 2.182003\n",
      "Epoch 78, loss: 2.167990\n",
      "Epoch 79, loss: 2.182103\n",
      "Epoch 80, loss: 2.175570\n",
      "Epoch 81, loss: 2.140747\n",
      "Epoch 82, loss: 2.163818\n",
      "Epoch 83, loss: 2.170105\n",
      "Epoch 84, loss: 2.177526\n",
      "Epoch 85, loss: 2.197057\n",
      "Epoch 86, loss: 2.167805\n",
      "Epoch 87, loss: 2.182810\n",
      "Epoch 88, loss: 2.181312\n",
      "Epoch 89, loss: 2.188457\n",
      "Epoch 90, loss: 2.157779\n",
      "Epoch 91, loss: 2.192993\n",
      "Epoch 92, loss: 2.209380\n",
      "Epoch 93, loss: 2.204594\n",
      "Epoch 94, loss: 2.190671\n",
      "Epoch 95, loss: 2.170990\n",
      "Epoch 96, loss: 2.175288\n",
      "Epoch 97, loss: 2.150001\n",
      "Epoch 98, loss: 2.179286\n",
      "Epoch 99, loss: 2.164571\n",
      "Accuracy after training for 100 epochs:  0.226\n",
      "Epoch 0, loss: 2.171382\n",
      "Epoch 1, loss: 2.120761\n",
      "Epoch 2, loss: 2.162342\n",
      "Epoch 3, loss: 2.145936\n",
      "Epoch 4, loss: 2.173681\n",
      "Epoch 5, loss: 2.203891\n",
      "Epoch 6, loss: 2.190578\n",
      "Epoch 7, loss: 2.167809\n",
      "Epoch 8, loss: 2.169680\n",
      "Epoch 9, loss: 2.170261\n",
      "Epoch 10, loss: 2.161837\n",
      "Epoch 11, loss: 2.191472\n",
      "Epoch 12, loss: 2.208669\n",
      "Epoch 13, loss: 2.168063\n",
      "Epoch 14, loss: 2.158622\n",
      "Epoch 15, loss: 2.180697\n",
      "Epoch 16, loss: 2.171559\n",
      "Epoch 17, loss: 2.144054\n",
      "Epoch 18, loss: 2.151792\n",
      "Epoch 19, loss: 2.171664\n",
      "Epoch 20, loss: 2.211542\n",
      "Epoch 21, loss: 2.183207\n",
      "Epoch 22, loss: 2.163094\n",
      "Epoch 23, loss: 2.172139\n",
      "Epoch 24, loss: 2.175770\n",
      "Epoch 25, loss: 2.200830\n",
      "Epoch 26, loss: 2.193582\n",
      "Epoch 27, loss: 2.170238\n",
      "Epoch 28, loss: 2.160918\n",
      "Epoch 29, loss: 2.180273\n",
      "Epoch 30, loss: 2.139193\n",
      "Epoch 31, loss: 2.213331\n",
      "Epoch 32, loss: 2.165426\n",
      "Epoch 33, loss: 2.159857\n",
      "Epoch 34, loss: 2.179437\n",
      "Epoch 35, loss: 2.160295\n",
      "Epoch 36, loss: 2.198219\n",
      "Epoch 37, loss: 2.191638\n",
      "Epoch 38, loss: 2.184543\n",
      "Epoch 39, loss: 2.191949\n",
      "Epoch 40, loss: 2.157114\n",
      "Epoch 41, loss: 2.166297\n",
      "Epoch 42, loss: 2.219867\n",
      "Epoch 43, loss: 2.159231\n",
      "Epoch 44, loss: 2.161794\n",
      "Epoch 45, loss: 2.171856\n",
      "Epoch 46, loss: 2.183354\n",
      "Epoch 47, loss: 2.181758\n",
      "Epoch 48, loss: 2.153427\n",
      "Epoch 49, loss: 2.145488\n",
      "Epoch 50, loss: 2.158181\n",
      "Epoch 51, loss: 2.175458\n",
      "Epoch 52, loss: 2.150189\n",
      "Epoch 53, loss: 2.175774\n",
      "Epoch 54, loss: 2.129654\n",
      "Epoch 55, loss: 2.155623\n",
      "Epoch 56, loss: 2.186719\n",
      "Epoch 57, loss: 2.170051\n",
      "Epoch 58, loss: 2.170621\n",
      "Epoch 59, loss: 2.154645\n",
      "Epoch 60, loss: 2.174883\n",
      "Epoch 61, loss: 2.182432\n",
      "Epoch 62, loss: 2.156414\n",
      "Epoch 63, loss: 2.192123\n",
      "Epoch 64, loss: 2.172835\n",
      "Epoch 65, loss: 2.186917\n",
      "Epoch 66, loss: 2.159821\n",
      "Epoch 67, loss: 2.153771\n",
      "Epoch 68, loss: 2.176289\n",
      "Epoch 69, loss: 2.189926\n",
      "Epoch 70, loss: 2.164574\n",
      "Epoch 71, loss: 2.198162\n",
      "Epoch 72, loss: 2.155483\n",
      "Epoch 73, loss: 2.139499\n",
      "Epoch 74, loss: 2.187601\n",
      "Epoch 75, loss: 2.171229\n",
      "Epoch 76, loss: 2.189153\n",
      "Epoch 77, loss: 2.173533\n",
      "Epoch 78, loss: 2.155557\n",
      "Epoch 79, loss: 2.186287\n",
      "Epoch 80, loss: 2.155610\n",
      "Epoch 81, loss: 2.135399\n",
      "Epoch 82, loss: 2.179988\n",
      "Epoch 83, loss: 2.173783\n",
      "Epoch 84, loss: 2.115271\n",
      "Epoch 85, loss: 2.176920\n",
      "Epoch 86, loss: 2.203259\n",
      "Epoch 87, loss: 2.183007\n",
      "Epoch 88, loss: 2.182428\n",
      "Epoch 89, loss: 2.132824\n",
      "Epoch 90, loss: 2.173451\n",
      "Epoch 91, loss: 2.163025\n",
      "Epoch 92, loss: 2.180528\n",
      "Epoch 93, loss: 2.199303\n",
      "Epoch 94, loss: 2.162961\n",
      "Epoch 95, loss: 2.190785\n",
      "Epoch 96, loss: 2.226618\n",
      "Epoch 97, loss: 2.183025\n",
      "Epoch 98, loss: 2.206076\n",
      "Epoch 99, loss: 2.178657\n",
      "Accuracy after training for 100 epochs:  0.226\n",
      "best validation accuracy achieved: 0.227000\n",
      "CPU times: user 4min 55s, sys: 31.8 s, total: 5min 27s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "best_loss_history = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for rate in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=100, learning_rate=rate, batch_size=300, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "            best_loss_history = loss_history\n",
    "        print(\"Accuracy after training for 100 epochs: \", accuracy)\n",
    "        \n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x118b85dd8>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.201000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
